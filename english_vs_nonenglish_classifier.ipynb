{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Corpus Loading and Splitting\n",
    "we’ll use the Universal Declaration of Human Rights (UDHR) corpus that comes with NLTK. We’ll select English as our target language and Farsi as our non-English sample, split the texts into sentences, label them accordingly, and then perform an 80/20 random split for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package udhr to\n",
      "[nltk_data]     /Users/majidtavakoli/nltk_data...\n",
      "[nltk_data]   Package udhr is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/majidtavakoli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import udhr\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download required resources if not already available\n",
    "nltk.download('udhr')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the UDHR texts for English and German\n",
    "english_text = udhr.raw('English-Latin1')\n",
    "german_text = udhr.raw('German_Deutsch-Latin1')\n",
    "\n",
    "# Split texts into sentences\n",
    "english_sentences = sent_tokenize(english_text)\n",
    "german_sentences = sent_tokenize(german_text)\n",
    "\n",
    "# Label the sentences: English as 'English' and German as 'Non-English'\n",
    "data = [(sent, 'English') for sent in english_sentences] + [(sent, 'Non-English') for sent in german_sentences]\n",
    "\n",
    "# Shuffle the data to randomize the order\n",
    "random.shuffle(data)\n",
    "\n",
    "# Split data into 80% training and 20% testing\n",
    "split_point = int(0.8 * len(data))\n",
    "train_data = data[:split_point]\n",
    "test_data = data[split_point:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Preprocessing and Feature Extraction\n",
    "A simple bag-of-words approach is used here. We define a function that tokenizes the text and creates a dictionary of words as features. Each word is lowercased and marked as present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # Create a feature dictionary: each word is a feature with value True\n",
    "    return {word.lower(): True for word in words}  # Lowercasing helps normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Training the Naïve Bayes Classifier\n",
    "We now train the classifier using NLTK’s built-in Naïve Bayes implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                       a = True           Englis : Non-En =      6.8 : 1.0\n",
      "                     the = None           Non-En : Englis =      5.1 : 1.0\n",
      "                      to = None           Non-En : Englis =      4.6 : 1.0\n",
      "                     und = None           Englis : Non-En =      2.4 : 1.0\n",
      "                     and = None           Non-En : Englis =      2.4 : 1.0\n",
      "                      of = None           Non-En : Englis =      2.3 : 1.0\n",
      "                       , = None           Englis : Non-En =      2.1 : 1.0\n",
      "                 article = None           Non-En : Englis =      2.0 : 1.0\n",
      "                     das = None           Englis : Non-En =      1.9 : 1.0\n",
      "                   recht = None           Englis : Non-En =      1.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Create feature sets for training and testing data\n",
    "train_features = [(extract_features(text), label) for (text, label) in train_data]\n",
    "test_features = [(extract_features(text), label) for (text, label) in test_data]\n",
    "\n",
    "# Train the Naïve Bayes classifier on the training features\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_features)\n",
    "\n",
    "# Display the most informative features\n",
    "classifier.show_most_informative_features(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Evaluation: Accuracy, Confusion Matrix, Precision, and Recall\n",
    "After training, evaluate the classifier’s performance on the test set. We calculate accuracy, generate a confusion matrix, and compute precision and recall for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Confusion Matrix:\n",
      "            |     N |\n",
      "            |     o |\n",
      "            |     n |\n",
      "            |     - |\n",
      "            |  E  E |\n",
      "            |  n  n |\n",
      "            |  g  g |\n",
      "            |  l  l |\n",
      "            |  i  i |\n",
      "            |  s  s |\n",
      "            |  h  h |\n",
      "------------+-------+\n",
      "    English |<10> . |\n",
      "Non-English |  .<12>|\n",
      "------------+-------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "English - Precision: 1.00, Recall: 1.00\n",
      "Non-English - Precision: 1.00, Recall: 1.00\n"
     ]
    }
   ],
   "source": [
    "from nltk import ConfusionMatrix\n",
    "from nltk.metrics import precision, recall\n",
    "\n",
    "# Evaluate accuracy on the test set\n",
    "accuracy = nltk.classify.accuracy(classifier, test_features)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Get the actual and predicted labels for the test set\n",
    "actual = [label for (_, label) in test_features]\n",
    "predicted = [classifier.classify(features) for (features, _) in test_features]\n",
    "\n",
    "# Create and print the confusion matrix\n",
    "cm = ConfusionMatrix(actual, predicted)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Prepare sets for calculating precision and recall\n",
    "actual_sets = {'English': set(), 'Non-English': set()}\n",
    "predicted_sets = {'English': set(), 'Non-English': set()}\n",
    "\n",
    "for index, (act, pred) in enumerate(zip(actual, predicted)):\n",
    "    actual_sets[act].add(index)\n",
    "    predicted_sets[pred].add(index)\n",
    "\n",
    "# Calculate precision and recall for each label\n",
    "for label in ['English', 'Non-English']:\n",
    "    p = precision(actual_sets[label], predicted_sets[label])\n",
    "    r = recall(actual_sets[label], predicted_sets[label])\n",
    "    print(f'{label} - Precision: {p:.2f}, Recall: {r:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
