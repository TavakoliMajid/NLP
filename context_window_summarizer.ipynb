{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Measure the Document Length\n",
    "This function tokenizes the input text using NLTKâ€™s word tokenizer and returns the number of tokens in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 6\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def measure_length(text):\n",
    "    \"\"\"Measure the length of the text in tokens.\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# Example usage:\n",
    "sample_text = \"This is a sample document.\"\n",
    "print(\"Token count:\", measure_length(sample_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Compute Target Summary Length\n",
    "This function computes the target summary length as a proportion of the total token count (e.g., 30% of the original text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target summary length: 300\n"
     ]
    }
   ],
   "source": [
    "def compute_target_length(total_length, target_ratio=0.3):\n",
    "    \"\"\"\n",
    "    Compute the target summary length based on a ratio.\n",
    "    For example, a ratio of 0.3 means the summary should have about 30% of the original tokens.\n",
    "    \"\"\"\n",
    "    return int(total_length * target_ratio)\n",
    "\n",
    "# Example usage:\n",
    "total_tokens = 1000\n",
    "print(\"Target summary length:\", compute_target_length(total_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Slice the Document\n",
    "This function slices a document into smaller chunks so that each chunk does not exceed the specified token limit (e.g., 4000 tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 5\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def slice_document(text, token_limit=4000):\n",
    "    \"\"\"\n",
    "    Slice the document into chunks where each chunk has at most token_limit tokens.\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    slices = []\n",
    "    for i in range(0, len(words), token_limit):\n",
    "        chunk = \" \".join(words[i:i+token_limit])\n",
    "        slices.append(chunk)\n",
    "    return slices\n",
    "\n",
    "# Example usage:\n",
    "sample_text = \" \".join([\"word\"] * 5000)  # A sample text with 5000 words.\n",
    "chunks = slice_document(sample_text, token_limit=1000)\n",
    "print(\"Number of chunks:\", len(chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Summarize a Text Slice\n",
    "This function uses a frequency-based approach to summarize a text slice. It tokenizes the text into sentences, calculates word frequencies (ignoring stopwords and punctuation), scores each sentence, and then selects the top-scoring sentences based on the specified summary ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: And yet another sentence to summarize.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def summarize_slice(text, summary_ratio=0.3):\n",
    "    \"\"\"\n",
    "    Summarize the text slice by:\n",
    "    - Tokenizing the text into sentences.\n",
    "    - Computing word frequencies (ignoring stopwords and punctuation).\n",
    "    - Scoring each sentence based on the frequencies of its words.\n",
    "    - Selecting the top sentences to form the summary.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return text\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    punctuations = set(string.punctuation)\n",
    "\n",
    "    words = word_tokenize(text.lower())\n",
    "    filtered_words = [word for word in words if word not in stop_words and word not in punctuations]\n",
    "    freq = nltk.FreqDist(filtered_words)\n",
    "\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        sentence_words = word_tokenize(sentence.lower())\n",
    "        score = sum(freq[word] for word in sentence_words if word in freq)\n",
    "        sentence_scores[sentence] = score\n",
    "\n",
    "    summary_length = max(1, int(len(sentences) * summary_ratio))\n",
    "    top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:summary_length]\n",
    "    top_sentences.sort(key=lambda s: sentences.index(s))\n",
    "    \n",
    "    summary = \" \".join(top_sentences)\n",
    "    return summary\n",
    "\n",
    "# Example usage:\n",
    "sample_text = (\"This is a sentence. \" \n",
    "               \"Here is another sentence. \" \n",
    "               \"And yet another sentence to summarize.\")\n",
    "print(\"Summary:\", summarize_slice(sample_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Hierarchical Summarization\n",
    "This function implements the hierarchical summarization strategy. It first checks whether the text fits within the token limit. If not, it slices the document into chunks, summarizes each chunk, collates these summaries, and then recursively summarizes the collated summary until the final summary meets the token limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical Summary: This is a long text for summarization . This is a long text for summarization .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Import functions from previous steps (assume they are defined in the notebook cells)\n",
    "# Here, we assume slice_document, summarize_slice, and measure_length are already defined.\n",
    "\n",
    "def hierarchical_summarize(text, token_limit=4000, summary_ratio=0.3):\n",
    "    \"\"\"\n",
    "    Perform hierarchical summarization:\n",
    "    - If the text is within the token limit, return it as is.\n",
    "    - Otherwise, slice the text into chunks that fit within the context window.\n",
    "    - Summarize each slice.\n",
    "    - Collate the summaries and, if necessary, summarize the collated summary recursively until it fits.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    if len(tokens) <= token_limit:\n",
    "        return text\n",
    "\n",
    "    # Slice the document into manageable chunks.\n",
    "    slices = slice_document(text, token_limit)\n",
    "    intermediate_summaries = [summarize_slice(chunk, summary_ratio) for chunk in slices]\n",
    "    \n",
    "    collated_summary = \" \".join(intermediate_summaries)\n",
    "    \n",
    "    # Recursively summarize if needed.\n",
    "    if measure_length(collated_summary) > token_limit:\n",
    "        return hierarchical_summarize(collated_summary, token_limit, summary_ratio)\n",
    "    else:\n",
    "        return collated_summary\n",
    "\n",
    "# Example usage:\n",
    "sample_text = \"This is a long text for summarization. \" * 1000\n",
    "summary = hierarchical_summarize(sample_text, token_limit=200, summary_ratio=0.3)\n",
    "print(\"Hierarchical Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Save Text to File\n",
    "A simple utility function to save text to a file. This is used for saving the final summaries and the generated query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text saved to output.txt\n"
     ]
    }
   ],
   "source": [
    "def save_text(filename, text):\n",
    "    \"\"\"Save the provided text to a file.\"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "# Example usage:\n",
    "sample_text = \"This text will be saved to a file.\"\n",
    "save_text(\"output.txt\", sample_text)\n",
    "print(\"Text saved to output.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Generate a Query\n",
    "This function generates a query by combining the summaries from the two documents. It extracts the most frequent keywords (after removing stopwords and punctuation) and joins them to form a simple query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Query: summary first second\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def generate_query(summary1, summary2, top_n=5):\n",
    "    \"\"\"\n",
    "    Generate a query by combining both summaries and selecting the top_n most frequent keywords.\n",
    "    \"\"\"\n",
    "    combined_text = summary1 + \" \" + summary2\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(combined_text.lower())\n",
    "    filtered_words = [word for word in words if word not in stop_words and word not in string.punctuation]\n",
    "    freq = nltk.FreqDist(filtered_words)\n",
    "    common_words = [word for word, count in freq.most_common(top_n)]\n",
    "    query = \" \".join(common_words)\n",
    "    return query\n",
    "\n",
    "# Example usage:\n",
    "summary1 = \"This is the first summary.\"\n",
    "summary2 = \"This is the second summary.\"\n",
    "print(\"Generated Query:\", generate_query(summary1, summary2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Main Execution\n",
    "This cell ties all the steps together. It measures the document lengths, computes target summary lengths (for informational purposes), performs hierarchical summarization on two documents, saves the summaries, and finally generates a query based on the summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 Length: 23\n",
      "Document 2 Length: 25\n",
      "Target Summary Length for Document 1 (approx.): 6\n",
      "Target Summary Length for Document 2 (approx.): 7\n",
      "\n",
      "Summary for Document 1:\n",
      " Your very long text for document 1 goes here. \n",
      "This document could be thousands of tokens long, requiring hierarchical summarization...\n",
      "\n",
      "Summary for Document 2:\n",
      " Your reference style text for document 2 goes here. \n",
      "This text is used to guide the style of the summary for document 1.\n",
      "\n",
      "Generated Query: document text long 1 goes\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Make sure required NLTK data is downloaded (uncomment if needed)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Assume the functions from previous steps are already defined in your notebook:\n",
    "# - measure_length\n",
    "# - compute_target_length\n",
    "# - slice_document\n",
    "# - summarize_slice\n",
    "# - hierarchical_summarize\n",
    "# - save_text\n",
    "# - generate_query\n",
    "\n",
    "# Example documents (replace these with your actual texts)\n",
    "document1 = \"\"\"Your very long text for document 1 goes here. \n",
    "This document could be thousands of tokens long, requiring hierarchical summarization...\"\"\"\n",
    "\n",
    "document2 = \"\"\"Your reference style text for document 2 goes here. \n",
    "This text is used to guide the style of the summary for document 1.\"\"\"\n",
    "\n",
    "# Step 1: Measure document lengths.\n",
    "length1 = measure_length(document1)\n",
    "length2 = measure_length(document2)\n",
    "print(\"Document 1 Length:\", length1)\n",
    "print(\"Document 2 Length:\", length2)\n",
    "\n",
    "# Step 2: Compute target summary lengths (informational).\n",
    "target_length1 = compute_target_length(length1)\n",
    "target_length2 = compute_target_length(length2)\n",
    "print(\"Target Summary Length for Document 1 (approx.):\", target_length1)\n",
    "print(\"Target Summary Length for Document 2 (approx.):\", target_length2)\n",
    "\n",
    "# Steps 3-7: Hierarchically summarize each document.\n",
    "summary1 = hierarchical_summarize(document1, token_limit=4000, summary_ratio=0.3)\n",
    "summary2 = hierarchical_summarize(document2, token_limit=4000, summary_ratio=0.3)\n",
    "print(\"\\nSummary for Document 1:\\n\", summary1)\n",
    "print(\"\\nSummary for Document 2:\\n\", summary2)\n",
    "\n",
    "# Step 6: Save the final summaries.\n",
    "save_text(\"final_summary_doc1.txt\", summary1)\n",
    "save_text(\"final_summary_doc2.txt\", summary2)\n",
    "\n",
    "# Step 7: Generate a query based on the two summaries.\n",
    "query = generate_query(summary1, summary2)\n",
    "save_text(\"generated_query.txt\", query)\n",
    "print(\"\\nGenerated Query:\", query)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
